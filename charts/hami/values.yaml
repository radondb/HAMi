# Default values for hami-vgpu.

nameOverride: ""
fullnameOverride: ""
imagePullSecrets: []
version: "v2.3.9.1"

#Nvidia GPU Parameters
resourceName: "qingcloud.nvidia.com/vgpu"
resourceMem: "qingcloud.nvidia.com/vgpumem"
resourceMemPercentage: "qingcloud.nvidia.com/vgpumem-percentage"
resourceCores: "qingcloud.nvidia.com/vgpucores"
resourcePriority: "qingcloud.nvidia.com/vgpupriority"

#MLU Parameters
mluResourceName: "cambricon.com/mlunum"
mluResourceMem: "cambricon.com/mlumem"

#Hygon DCU Parameters
dcuResourceName: "qingcloud.hygon.com/dcunum"
dcuResourceMem: "qingcloud.hygon.com/dcumem"
dcuResourceCores: "qingcloud.hygon.com/dcucores"

#Iluvatar GPU Parameters
iluvatarResourceName: "iluvatar.ai/vgpu"
iluvatarResourceMem: "iluvatar.ai/vcuda-memory"
iluvatarResourceCore: "iluvatar.ai/vcuda-core"

schedulerName: "hami-scheduler"

podSecurityPolicy:
  enabled: false

global:
  gpuHookPath: /usr/local
  labels: {}
  annotations: {}

scheduler:
  # @param nodeName defines the node name and the nvidia-vgpu-scheduler-scheduler will schedule to the node.
  # if we install the nvidia-vgpu-scheduler-scheduler as default scheduler, we need to remove the k8s defualt
  # scheduler pod from the cluster first, we must specified node name to skip the schedule workflow.
  nodeName: ""
  defaultMem: 0
  defaultCores: 0
  defaultGPUNum: 1
  metricsBindAddress: ":9395"
  kubeScheduler:
    # @param enabled indicate whether to run kube-scheduler container in the scheduler pod, it's true by default.
    enabled: true
    imageTag: "v1.26.5"
    image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler
    imagePullPolicy: IfNotPresent
    extraNewArgs:
      - --config=/config/config.yaml
      - -v=5
    extraArgs:
      - --policy-config-file=/config/config.json
      - --leader-elect=false
      - -v=5
  extender:
    image: "radondb/hami"
    imagePullPolicy: IfNotPresent
    extraArgs:
      - --debug
      - -v=5
  podAnnotations: {}
  #nodeSelector:
  #  gpu: "on"
  tolerations: []
  #serviceAccountName: "hami-vgpu-scheduler-sa"
  customWebhook:
    enabled: false
    # must be an endpoint using https.
    # should generate host certs here
    host: 127.0.0.1 # hostname or ip, can be your node'IP if you want to use https://<nodeIP>:<schedulerPort>/<path>
    port: 31998
    path: /webhook
  patch:
    image: docker.io/jettech/kube-webhook-certgen:v1.5.2
    imageNew: liangjw/kube-webhook-certgen:v1.1.1
    imagePullPolicy: IfNotPresent
    priorityClassName: ""
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    runAsUser: 2000
  mutatingWebhookConfiguration:
    failurePolicy: Ignore
  service:
    httpPort: 443
    schedulerPort: 31998
    monitorPort: 31993
    labels: {}
    annotations: {}

devicePlugin:
  image: "radondb/hami"

  monitorimage: "radondb/hami"
  monitorctrPath: /usr/local/vgpu/containers
  imagePullPolicy: IfNotPresent
  deviceSplitCount: 100
  deviceMemoryScaling: 0.8
  deviceCoreScaling: 2
  runtimeClassName: ""
  migStrategy: "none"
  disablecorelimit: "false"
  extraArgs:
    - --pass-device-specs=true
    - -v=false

  hygonimage: "radondb/hami:v2.3.9.1"
  hygondriver: "/root/dtk-24.04"
  
  service:
    httpPort: 31992
    
  pluginPath: /var/lib/kubelet/device-plugins
  libPath: /usr/local/vgpu

  podAnnotations: {}
  nvidianodeSelector:
    aicp.group/aipods_type: "vGPU"
  mlunodeSelector:
    mlu: "on"
  hygonnodeSelector:
    dcu: "on"
  tolerations:
    - key: "aicp.group/worker"
      operator: "Exists"
    - key: "aicp.group/resource_group"
      operator: "Exists"

